{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stuff.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-217f1b5ade23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stuff.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-217f1b5ade23>\u001b[0m in \u001b[0;36mfun\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Opening the file in read mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Creating an empty dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stuff.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Problem A2. Calculation of word frequency from the givel corpus/text file\n",
    "\"\"\"\n",
    "\n",
    "def word_freq(filename):\n",
    "    import string\n",
    "    # Opening the file in read mode\n",
    "    doc = open(filename , \"r\")\n",
    "\n",
    "    # Creating an empty dictionary\n",
    "    d = dict()\n",
    "\n",
    "    # Looping through the file\n",
    "    for line in doc:\n",
    "        # Removing the spaces and the newline character\n",
    "        line = line.strip()\n",
    "\n",
    "        # Convert the characters in line to lowercase\n",
    "        line = line.lower()\n",
    "\n",
    "        # Removing the punctuation marks\n",
    "        line = line.translate(line.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        # Splitting lines into words\n",
    "        words = line.split(\" \")\n",
    "\n",
    "        # looping over each word in line\n",
    "        for word in words:\n",
    "            \n",
    "        # Checking if the word is already present in dictionary\n",
    "            if word in d:\n",
    "            # Increment count\n",
    "                d[word] = d[word] + 1\n",
    "            else:\n",
    "            # Add the word to dictionary with count = 1\n",
    "                d[word] = 1\n",
    "        #returning frequency of terms\n",
    "        return d\n",
    "\n",
    "\n",
    "d = word_freq(\"sample_file.txt\") \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-55b5a46ffcd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'_main_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;31m#taking input of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Probelm A 3- Lazy distance\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def util(a,b,a_i,b_i):\n",
    "    #cAatt Cat\n",
    "    if(b_i==0 and a_i==0):\n",
    "        return 0\n",
    "    if(b_i!=0 and a_i==0):\n",
    "        return -1\n",
    "    if(b_i==0):\n",
    "        return a_i\n",
    "    \n",
    "     #if s==s return 0+ if last two characters are same(with same case also) check for 2nd last char\n",
    "    if(a[a_i-1]==b[b_i-1]):\n",
    "        ans=util(a,b,a_i-1,b_i-1)\n",
    "        if(ans==-1):\n",
    "            return -1\n",
    "        return ans\n",
    "    \n",
    "   #if s==S (if character same but case different,do return 1+ check further characters , try from deleting character from a and compare answers from both cases)\n",
    "    if(a[a_i-1]==b[b_i-1].lower() or a[a_i-1]==b[b_i-1].upper()):\n",
    "        ans=util(a,b,a_i-1,b_i-1)\n",
    "        ans1=util(a,b,a_i-1,b_i)\n",
    "        if(ans==-1 and ans1==-1):\n",
    "            return -1\n",
    "        if(ans==-1 or ans1==-1):\n",
    "            return 1+max(ans,ans1)\n",
    "        return 1+min(ans,ans1)\n",
    "    \n",
    "    #if s==t return 1+delete from string a\n",
    "    ans=util(a,b,a_i-1,b_i)\n",
    "    if(ans==-1):\n",
    "        return -1\n",
    "    return 1+ans\n",
    "    \n",
    "def editDistance(a, b):\n",
    "    return util(a,b,len(a),len(b))\n",
    "\n",
    "\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    #taking input of words\n",
    "    T=int(input())\n",
    "    for i in range(T):\n",
    "        s, t = input().split()\n",
    "        ans = editDistance(s, t)\n",
    "        print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProblem A3 : The problem I could try to solve by making both the string to lowercase and as I have the possibility \\nto delete only the first string characters thus I will focus on first string to manipulate and try to match with 2nd string.\\n\\nAnd as I can delete one or many characters from first string, thus i would try to delete characters one by one\\nfrom it and match it from the second one. Each deleting operation I will count and return as lazy \\ndistance otherwise to -1 as not a match.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Problem A3 : The problem I could try to solve by making both the string to lowercase and as I have the possibility \n",
    "to delete only the first string characters thus I will focus on first string to manipulate and try to match with 2nd string.\n",
    "\n",
    "And as I can delete one or many characters from first string, thus i would try to delete characters one by one\n",
    "from it and match it from the second one. Each deleting operation I will count and return as lazy \n",
    "distance otherwise to -1 as not a match.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problem B1\n",
    "\n",
    "The problem has three parts:\n",
    "1. To recognize and correct the misspelled word.\n",
    "2. To classify the political orientation of speech\n",
    "3. Topic summary taking all speeches together \n",
    "\n",
    "1. For first problem the preprocessed data is given where the words are categorized as True and false for the spelling.\n",
    "If the Spellinglabel is false, that word can selectively be corrected with the help of any spell checker liberary function.\n",
    "Here I have used TextBlob correct method.\n",
    "\n",
    "2. To classify the political orientation, first the speeched are converted into numeric matrics with the help of Tfidf vectorizer \n",
    "liberary. Before the the Speeched are cleaned, broken down into tokens, and stemmed to reduced the number of words and improve the efficiuency of the model.\n",
    "the tfidf matrix can be used then to find out the cosine similarity to cluster the orientation or used in classification \n",
    "model to cluster into three different categories.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\"\"\"\n",
    "speech_text is having preprocessed data with labels of the words, as which word is correct and which is not. \n",
    "The incorrect word is having label as false so when it occurs while iterating through the speech_text dataframe,\n",
    "the textblob correct method will replace the correct spelling with the incorrect one. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def spellcheck(speech_text):\n",
    "    label= speech_text[1]\n",
    "    for i in range(len(speech_text)):\n",
    "        if label[i] ==False:\n",
    "            b = TextBlob(speech_text[\"word\"][i])\n",
    "            #b.correct will modify the incorrect spelling to correct one\n",
    "            speech_text[\"word\"][i] = b.correct()\n",
    "    return speech_text\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-603853bb78d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#I am assuming speech_df is having text speeches of different parties from time to time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msent_tokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeech_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Word Tokenizing first sentence from sent_tokenized, saving as words_tokenized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speech_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizing a paragraph into sentences and storing in sent_tokenized\n",
    "#I am assuming speech_df is having text speeches of different parties from time to time \n",
    "\n",
    "sent_tokenized = [sent for sent in nltk.sent_tokenize(speech_df)]\n",
    "\n",
    "# Word Tokenizing first sentence from sent_tokenized, saving as words_tokenized\n",
    "words_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]\n",
    "\n",
    "# Removing tokens that do not contain any letters from words_tokenized\n",
    "import re\n",
    "regex = r\"[a-zA-Z]\"   #defining pattern to clean the text\n",
    "filtered_text = [word for word in words_tokenized if re.search(regex, word)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "\n",
    "# Create an English language SnowballStemmer object\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform both stemming and tokenization\n",
    "def tokenize_and_stem(text):\n",
    "    \n",
    "    # Tokenize by sentence, then by word\n",
    "    tokens = [word for word in nltk.word_tokenize(text)]\n",
    "    #print(\"tokens of text are: \", tokens)\n",
    "    \n",
    "    # Filter out raw tokens to remove noise\n",
    "    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]\n",
    "    #print(\"filtered tokens are :\", filtered_tokens)\n",
    "    # Stem the filtered_tokens\n",
    "    stems = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TfidfVectorizer to create TF-IDF vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# Instantiate TfidfVectorizer object with stopwords and tokenizer\n",
    "# parameters for efficient processing of text\n",
    "\n",
    "#Term frequency inverse document frequency will help us to short out the important words from the speech and remove \n",
    "#stopwords or repetetive non important words, which will help us to categorize different clusters of the speech text.\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem,\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-53e6532d8a9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspeech_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"speech\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'speech_df' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform([x for x in speech_df[\"speech\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps would be to find out the political orientation by defining a pool of words for extreme right neutral and right orientation, then to compare the speech if the speech has those words or not. Another thing can be think of by making clusters using K-Mean, keeping no of clusters as three. So the speech which are alike will go in cluster 0 and 1 and 2 which can be categorized as left neutral and right group. \n",
    "One more thing can be done to check this is by checking the cosine similarity of the speech text and categorizing speeches having similar cosine similarity into three separate groups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
